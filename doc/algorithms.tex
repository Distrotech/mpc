\documentclass {article}

\usepackage[a4paper]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb}

\newcommand {\mpc}{\texttt {mpc}}
\newcommand {\mpfr}{\texttt {mpfr}}
\newcommand {\ulp}[1]{#1~ulp}
\newcommand {\atantwo}{\operatorname {atan2}}
\newcommand {\Ulp}{{\rm ulp}}
\DeclareMathOperator{\error}{error}
\DeclareMathOperator{\Exp}{{\rm \textsc exp}}
\DeclareMathOperator{\pinf}{\bigtriangleup}
\DeclareMathOperator{\minf}{\bigtriangledown}
\DeclareMathOperator{\N}{\mathcal N}
\DeclareMathOperator{\A}{\mathcal A}
\newtheorem{lemma}{Lemma}
\def\eos{\vrule height7pt width 6pt depth 0pt} %% end-of-something
\newenvironment{proof}{\par\noindent{\bf Proof:}}{{\hfill\eos}\par\smallskip}

\title {MPC: Algorithms and Error Analysis}
\author {Andreas Enge \and Philippe Th\'eveny \and Paul Zimmermann}
\date {June 4, 2009}

\begin {document}
\maketitle
\tableofcontents


\section {Error analysis}

This section is devoted to the analysis of error propagation: Given a function
whose input arguments already have a certain error, what is the error bound on
the function output? The output error usually consists of two components: the
error propagated from the input, which may be arbitrarily amplified; and an
additional small error accounting for the rounding of the output. The results
are needed for algorithms that combine several arithmetic operations.

We will assume that the real and imaginary parts of the operands have the same
precision $p$. We write any nonzero real number $x$ in the form $x = m \cdot
2^e$ with $\frac{1}{2} \le |m| < 1$, and we define $\Exp(x) := e$ and $\Ulp(x)
:= 2^{\Exp(x) - p}$.  We will also assume that the two operands $z_1$ and
$z_2$ in the following operations are approximations of exact values
$\widetilde{z_1}$ and $\widetilde{z_2}$ respectively. For $n=1, 2$, let
$\widetilde{z_n} = \widetilde{x_n} + i \widetilde{y_n}$ and $z_n = x_n + i
y_n$, let $k_{R,n}$ and $k_{I,n}$ the upper bound coefficients: $\error(x_n)
\leq k_{R,n} \Ulp(x_n)$ and $\error(y_n) \leq k_{I,n} \Ulp(y_n)$.  Note that
we always have $|x_n|.c_{R,n}^- \leq |\widetilde{x_n}| \leq |x_n|.c_{R,n}^+$
and $|y_n|.c_{I,n}^- \leq |\widetilde{y_n}| \leq |y_n|.c_{I,n}^+$, with
$c_{\cdot,n}^- = 1-k_{\cdot,n}2^{1-p}$ and $c_{\cdot,n}^+ =
1+k_{\cdot,n}2^{1-p}$. When $z_n$ is obtained by rounding $\widetilde{z_n}$ to
the precision $p$ (we note $z_n=\circ(\widetilde{z_n})$), we have another
inequalities: $\frac{1}{2}|x_n| \leq |\widetilde{x_n}| \leq 2|x_n|$; if the
rounding mode is rounding towards plus infinity: $\widetilde{x_n} \leq x_n$,
and if the rounding mode is toward minus infinity: $x_n \leq \widetilde{x_n}$
(similar relations hold for the imaginary part).

Let $z=x+iy$ the result of the operation $z_1\diamond z_2$ rounded to the
precision of $z$ (we note $z=\circ(z_1 \diamond z_2)$). Let $c_R$ the bound
coefficient for the rounding error of the real part: $\error(x) = |x-\Re(z_1
\diamond z_2)| \leq c_R \Ulp(x)$, if the rounding mode is rounding to nearest
$c_R=\frac{1}{2}$, else $c_R=1$ (same relations for $c_I$, the coefficient for
the imaginary part, \emph{mutatis mutandis}).

\subsection {Generic error of addition/substraction}

Using the notations of the introductory paragraphs above, let
\[
z=\circ(z_1+z_2).
\]
We have
\begin{align*}
\error(x)&= |x-\Re(\widetilde{z_1}-\widetilde{z_2})|
\\
&\leq |x-\Re(z_1-z_2)|+|\Re(z_1+z_2)-\Re(\widetilde{z_1}+\widetilde{z_2})|
\\
&\leq c_R\Ulp(x)+|x_1-\widetilde{x_1}|+|x_2-\widetilde{x_2}|
\\
&\leq c_R\Ulp(x)+k_{R,1}\Ulp(x_1)+k_{R,2}\Ulp(x_2)
\\
&\leq \left[c_R+k_{R,1}2^{d_{R,1}}+k_{R,2}2^{d_{R,2}}\right]\Ulp(x)
\end{align*}
where $d_{R,n}=\Exp(x_n)-\Exp(x)$. If $x_1x_2 \geq 0$, then we have a simpler
expression
\[
\error(x) \leq [c_R+k_{R,1}+k_{R,2}] \Ulp(x).
\]
In the same way, we can show that the generic error of the imaginary part is
\[
\error (y) \leq \left[c_I+k_{I,1}2^{d_{I,1}}+k_{I,2}2^{d_{I,2}}\right] \Ulp(y)
\]
where $d_{I,n}=\Exp(y_n)-\Exp(y)$. If $y_1y_2 \geq 0$, then we have the
simpler expression
\[
\error(y) \leq [c_I+k_{I,1}+k_{I,2}] \Ulp(y).
\]


\subsection {Generic error of multiplication}

Using the notations of the introductory paragraphs above, let
\[
z=\circ(z_1\times z_2).
\]
We have
\[
\error(x) = |x-\Re(\widetilde{z_1}\times \widetilde{z_2})| \leq
|x-\Re(z_1\times z_2)|
+|\Re(z_1\times z_2)-\Re(\widetilde{z_1}\times\widetilde{z_2})|.
\]
The first term on the right hand side is the rounding error, so
\[
|x-\Re(z_1\times z_2)| \leq c_R \Ulp(x),
\]
the second term can be split as follows
\[
|\Re(z_1\times z_2)-\Re(\widetilde{z_1}\times\widetilde{z_2})| \leq
|x_1x_2-\widetilde{x_1}\widetilde{x_2}|
+|y_1y_2-\widetilde{y_1}\widetilde{y_2}|.
\]
Let us bound the part with real components:
\begin{align*}
|x_1x_2-\widetilde{x_1}\widetilde{x_2}| &\leq
\frac{1}{2}\left(|x_2||x_1-\widetilde{x_1}|
+|\widetilde{x_1}||x_2-\widetilde{x_2}|+|x_1||x_2-\widetilde{x_2}|
+|\widetilde{x_2}||x_1-\widetilde{x_1}|\right)
\\
&\leq \left[(1+c_{R,2})k_{R,1}+(1+c_{R,1})k_{R,2}\right] \Ulp(x_1x_2),
\end{align*}
where $c_{R,n}$ is such that $|\widetilde{x_n}| \leq c_{R,n}|x_n|$, it could
be $c_{R,n}^+=1+k_{R,n}2^{1-p}$ or even 1 if $x_n$ is $\widetilde{x_n}$ rounded
away from zero (see the introductory paragraphs above). In a similar way, we
have
\[
|y_1y_2-\widetilde{y_1}\widetilde{y_2}| \leq
\left[(1+c_{I,1})k_{I,2}+(1+c_{I,2})k_{I,1}\right] \Ulp(y_1y_2).
\]
If [$x_1x_2 \geq 0$ and $y_1y_2 \geq 0$] or [$x_1x_2 \leq 0$ and $y_1y_2 \leq
  0$] then $\Ulp(x_1x_2) \leq \Ulp(x)$ and $\Ulp(y_1y_2) \leq \Ulp(x)$, so we
have
\[
\error(x) \leq \left[c_R + (1+c_{R,1})k_{R,2} + (1+c_{R,2})k_{R,1} +
(1+c_{I,1})k_{I,2} + (1+c_{I,2})k_{I,1} \right] \Ulp(x).
\]
If $x_1x_2$ and $y_1y_2$ don't have the same sign, then the inequality is
\[
\error(x) \leq \left[ c_R + \left( (1+c_{R,1})k_{R,2} + (1+c_{R,2})k_{R,1}
\right)2^d + \left( (1+c_{I,1})k_{I,2} + (1+c_{I,2})k_{I,1} \right)2^{d'}
\right] \Ulp(x)
\]
where $d = \Exp(x_1x_2)-\Exp(x) \leq \Exp(x_1)+\Exp(x_2)-\Exp(x)$ and $d' =
\Exp(y_1y_2)-\Exp(x) \leq \Exp(y_1)+\Exp(y_2)-\Exp(x)$.

Error of the imaginary part can be bounded the same way. If $x_1y_2$ and
$x_2y_1$ {\bf do not have} the same sign, then
\[
\error(y) \leq \left[c_I + (1+c_{R,1})k_{I,2} + (1+c_{I,2})k_{R,1} +
(1+c_{I,1})k_{R,2} + (1+c_{R,2})k_{I,1}\right] \Ulp(y).
\]
If $x_1y_2$ and $y_1x_2$ {\bf have} the same sign, then
\[
\error(y) \leq \left[c_I + \left( (1+c_{R,1})k_{I,2} + (1+c_{I,2})k_{R,1}
\right)2^{\delta} + \left( (1+c_{I,1})k_{R,2} + (1+c_{R,2})k_{I,1} \right)
2^{\delta'}\right] \Ulp(y).
\]
where $\delta = \Exp(x_1y_2)-\Exp(y) \leq \Exp(x_1)+\Exp(y_2)-\Exp(y)$ and
$\delta' = \Exp(y_1x_2)-\Exp(y) \leq \Exp(y_1)+\Exp(x_2)-\Exp(y)$.

\subsection {Generic error of division}\label{generic:div}

Using the notations of the introductory paragraphs above, let
\[
z=\circ(\frac{z_1}{z_2}).
\]

We note
\begin{align*}
A&=\widetilde{x_1}\widetilde{x_2}+\widetilde{y_1}\widetilde{y_2}&
B&=\widetilde{y_1}\widetilde{x_2}-\widetilde{x_1}\widetilde{y_2}&
C&=\widetilde{x_2}^2+\widetilde{y_2}^2\\
a&=x_1x_2+y_1y_2&
b&=y_1x_2-x_1y_2&
c&=x_2^2+y_2^2
\end{align*}
then the error of the real part is
\[
\error(x) = \left|x-\frac{A}{C}\right| \leq \left|x-\frac{a}{c}\right| +
\left|\frac{a}{c}-\frac{A}{C}\right|.
\]
The first term of the right hand side is the rounding error:
\[
\left|x-\frac{a}{c}\right|\leq c_R \Ulp(x).
\]
The second term can be bounded as follows:
\[
\left|\frac{a}{c}-\frac{A}{C}\right| \leq \frac{1}{c} |a-A|
+\left|\frac{A}{C}\right|\frac{1}{c}|c-C|.
\]
We have
\[
|a-A| \leq |x_1x_2-\widetilde{x_1}\widetilde{x_2}|
+|y_1y_2-\widetilde{y_1}\widetilde{y_2}|,
\]
with
\begin{align*}
|x_1x_2-\widetilde{x_1}\widetilde{x_2}| &\leq \frac{1}{2} \left(
|x_1||x_2-\widetilde{x_2}|+|\widetilde{x_2}||x_1-\widetilde{x_1}|
+|x_2||x_1-\widetilde{x_1}|+|\widetilde{x_1}||x_2-\widetilde{x_2}| \right)\\
&\leq \left[ (1+c_{R,1})k_{R,2}+(1+c_{R,2})k_{R,1} \right] \Ulp(x_1x_2),
\end{align*}
where $c_{R,n}$ is such that $|\widetilde{x_n}| \leq c_{R,n} |x_n|$ (see the
introductory paragraphs above) and using the following general rule
\[
|u|\cdot\Ulp(v) \leq 2\Ulp(uv);
\]
let $d=\Exp(x_1x_2)-\Exp(a)$, we have
\[
|x_1x_2-\widetilde{x_1}\widetilde{x_2}| \leq
\left((1+c_{R,1})k_{R,2}+(1+c_{R,2})k_{R,1}\right)2^d \Ulp(a),
\]
and we can show in a similar way that
\[
|y_1y_2-\widetilde{y_1}\widetilde{y_2}| \leq \left( (1+c_{I,1})k_{I,2}
+(1+c_{I,2})k_{I,1} \right)2^{d'} \Ulp(a),
\]
where $d'=\Exp(y_1y_2)-\Exp(a)$. Because $x$ is $a/c$ rounded to the working
precision, we have $\Ulp(a/c) \leq \Ulp(x)$ and, as $c$ is positive,
$\Ulp(a)/c \leq 2\Ulp(a/c)$. Thus $\Ulp(a)/c \leq 2\Ulp(x)$. Using the
preceding inequalities we have
\[
\frac{1}{c}|a-A| \leq \left[
\left((1+c_{R,1})k_{R,2}+(1+c_{R,2})k_{R,1}\right)2^{1+d}
+\left((1+c_{I,1})k_{I,2}+(1+c_{I,2})k_{I,1}\right)2^{1+d'}
\right] \Ulp(x).
\]
In addition, we have
\[
|c-C| \leq \left|x_2^2-\widetilde{x_2}^2\right| +
\left|y_2^2-\widetilde{y_2}^2\right|,
\]
with
\begin{align*}
\left|x_2^2-\widetilde{x_2}^2\right| &= \left|x_2+\widetilde{x_2}\right|
\left|x_2-\widetilde{x_2}\right|\\ &\leq (1+c_{R,2})|x_2|k_{R,2}
\Ulp(x_2)\\ &\leq 2(1+c_{R,2})k_{R,2}\Ulp(x_2^2),
\end{align*}
we can show in a similar way that
\[
\left|y_2^2-\widetilde{y_2}^2\right| \leq 2(1+c_{I,2})k_{I,2}\Ulp(y_2^2).
\]
But $\Ulp(x_2^2) \leq \Ulp(x_2^2+y_2^2) = \Ulp(c)$, and the same relation
holds for $\Ulp(y_2^2)$, thus
\[
|c-C| \leq 2\left[(1+c_{R,2})k_{R,2}+(1+c_{I,2})k_{I,2}\right]\Ulp(c);
\]
let $c_{R,2}^-$ and $c_{I,2}^-$ two positive numbers such that $c_{R,2}^-|x_2|
\leq \left|\widetilde{x_2}\right|$ and $c_{I,2}^-|y_2| \leq
\left|\widetilde{y_2}\right|$, then
\[
\left|\frac{A}{C}\right| \leq
\frac{c_{R,1}c_{R,2}+c_{I,1}c_{I,2}}{(c_{R,2}^-)^2+(c_{I,2}^-)^2}
\left|\frac{a}{c}\right|,
\]
and, noticing that $\left|\frac{a}{c}\right|\frac{1}{c}\Ulp(c) \leq 2\Ulp(x)$,
we have
\[
\left|\frac{A}{C}\right|\frac{1}{c}|c-C| \leq
4\frac{c_{R,1}c_{R,2}+c_{I,1}c_{I,2}}{(c_{R,2}^-)^2+(c_{I,2}^-)^2}
\left((1+c_{R,2})k_{R,2}+(1+c_{I,2})k_{I,2}\right)\Ulp(x).
\]
Gathering the relevant inequalities, we show that the error on the real part
is bounded in the following way
\begin{equation*}
  \begin{split}
    \error(x) &\leq [c_R\\
    &\quad+\left((1+c_{R,1})k_{R,2}+(1+c_{R,2})k_{R,1}\right)2^{1+d}
    +\left((1+c_{I,1})k_{I,2}+(1+c_{I,2})k_{I,1}\right)2^{1+d'}\\
    &\quad+4\frac{c_{R,1}c_{R,2}+c_{I,1}c_{I,2}}{(c_{R,2}^-)^2+(c_{I,2}^-)^2}
    \left((1+c_{R,2})k_{R,2}+(1+c_{I,2})k_{I,2}\right)]
    \Ulp(x).
  \end{split}
\end{equation*}

An analog process gives
\begin{equation*}
  \begin{split}
    \error(y) &\leq [c_I\\
    &\quad+\left((1+c_{I,1})k_{R,2}+(1+c_{R,2})k_{I,1}\right)2^{1+\delta}
    +\left((1+c_{R,1})k_{I,2}+(1+c_{I,2})k_{R,1}\right)2^{1+\delta'}\\
    &\quad+4\frac{c_{R,1}c_{R,2}+c_{I,1}c_{I,2}}{(c_{R,2}^-)^2+(c_{I,2}^-)^2}
    \left((1+c_{R,2})k_{R,2}+(1+c_{I,2})k_{I,2}\right)]
    \Ulp(y),
  \end{split}
\end{equation*}
with $\delta=\Exp(y_1x_2)-\Exp(b)$ and $\delta'=\Exp(x_1y_2)-\Exp(b)$.

Note that, when $z_1$ and $z_2$ are the rounded values of $\widetilde{z_1}$
and $\widetilde{z_2}$ respectively, with rounding away from zero, then we can
substitute 1 for $k_{R,n}$, $c_{R,n}$, $k_{I,n}$, $c_{I,n}$ (with $n=1,2$),
$\frac{1}{2}$ for $c_{R,2}^-$, $c_{I,2}^-$, giving the much simpler
inequalities:
\begin{align*}
\error(x) &\leq [c_R + 2^{3+d} + 2^{3+d'} + 2^6] \Ulp(x)\\
\error(y) &\leq [c_I + 2^{3+\delta} + 2^{3+\delta'} + 2^6] \Ulp(y).
\end{align*}
If the rounding mode is rounding to nearest, then we can substitute
$\frac{1}{2}$ for $k_{R,n}$, $k_{I,n}$, $c_{R,2}^-$, $c_{I,2}^-$, 2 for
$c_{R,n}$, $c_{I,n}$ and we have:
\begin{align*}
\error(x) &\leq [c_R + 3\times 2^{1+d} +3\times 2^{1+d'} + 192] \Ulp(x)\\
\error(x) &\leq [c_R + 3\times 2^{1+\delta} +3\times 2^{1+\delta'} + 192]
\Ulp(y).
\end{align*}

At last, let us remark that $x_1x_2$ and $y_1y_2$ cannot have the same
sign if $y_1x_2$ and $-x_1y_2$ do have the same sign, thus there is
always a cancellation sometimes in real part, sometimes in imaginary
part of the division.


\section {Algorithms}

This section describes in detail the algorithms used in \mpc, together with the error analysis that allows to prove that the results are correct in the {\mpc} semantics: The input numbers are assumed to be exact, and the output corresponds to the exact result rounded in the desired direction.


\subsection {\texttt {mpc\_sqrt}}

The following algorithm is due to Friedland \cite{Friedland67,Smith98}.
Let $z = x + i y$.

Let $w = \sqrt { \frac {|x| + \sqrt {x^2 + y^2}}{2}}$ and
$t = \frac {y}{2w}$. Then $(w + it)^2 = |x| + iy$, and with the branch cut on the negative real axis we obtain
\[
\sqrt z = \left\{
\begin {array}{cl}
w + i t & \text {if } x > 0 \\
t + i w & \text {if } x < 0, y > 0 \\
-t - i w & \text {if } x < 0, y < 0
\end {array}
\right.
\]

$w$ is rounded down. $\sqrt {x^2 + y^2}$ is computed with an error of \ulp{1}; $|x|$ is added with an error of \ulp{1}, since both terms are positive. The generic error of the real square root in the special case that the argument was rounded down is \ulp{1}, so that the total error in computing $w$ is \ulp{3}.

$t$ is rounded up. The generic error of real division, applied to an error of \ulp{3} for $w$ and \ulp{0} for $y$ implies an error of \ulp{7}.


\subsection {\texttt {mpc\_log}}

Let $z = x + i y$. Then $\log (z) = \frac {1}{2} \log (x^2 + y^2) + i \atantwo (y, x)$. The imaginary part is computed by a call to the corresponding {\mpfr} function.

Let $w = \log (x^2 + y^2)$, rounded down. The error of the complex norm is \ulp{1}. The generic error of the real logarithm is then given by \ulp{$2^{2 - e_w} + 1$}, where $e_w$ is the exponent of $w$. For $e_w \geq 2$, this is bounded by \ulp{2} or 2~digits; otherwise, it is bounded by \ulp{$2^{3 - e_w}$} or $3 - e_w$ digits.

\subsection {\texttt {mpc\_tan}}

Let $z = x + i y$ with $x \neq 0$ and $y \neq 0$.

We compute $\tan z$ as follows:
\begin{align*}
u &\leftarrow \A(\sin z) &\error(\Re(u)) &\leq 1 \Ulp(\Re(u))
&\error(\Im(u)) &\leq 1 \Ulp(\Im(u))
\\
v &\leftarrow \A(\cos z) &\error(\Re(v)) &\leq 1 \Ulp(\Re(v))
&\error(\Im(v)) &\leq 1 \Ulp(\Im(v))
\\
t &\leftarrow \A(u/v) &\error(\Re(t)) &\leq k_R \Ulp(\Re(t))
&\error(\Im(t)) &\leq k_I \Ulp(\Im(t))
\end{align*}
where $w_2 \leftarrow \A(w_1)$ means that the real and imaginary parts of
$w_2$ are respectively the real and imaginary part of $w_1$ rounded away from
zero to the working precision.

We know that $\Re(\frac{a+i b}{c+i d})=\frac{a c +b d}{c^2 + d^2}$ and
$\Im(\frac{a+i b}{c+i d})=\frac{a d -b c}{c^2 + d^2}$, so in the special case
of $\tan z=\frac{\sin x\cosh y+i\cos x\sinh y}{\cos x\cosh y-i\sin x\sinh y}$,
we have $abcd < 0$ which means that there might be a cancellation in the
computation of the real part while it does never happen in the one of the
imaginary part.  Then, using the generic error of the division (see
\ref{generic:div}), we have
\begin{align*}
\error(\Re(t)) &\leq [1+2^{3+e_1}+2^{3+e_2}+2^6] \Ulp(\Re(t)),
\\
\error(\Im(t)) &\leq [1+2^3+2^3+2^6] \Ulp(\Im(t)),
\end{align*}
where $e_1=\Exp(a c) -\Exp(a c+b d)$ and $e_2=\Exp(b d) -\Exp(a c+b d)$.  The
second inequality shows that $2^7$ is suitable choice for $k_I$. As $|\sinh
y|<\cosh y$ for every nonzero $y$, we have $bd<ac$, thus $e_2\leq e_1$. We
know that $\Exp(\frac{a c+b d}{c^2+d^2})\leq \Exp(a c+b d) -\Exp(c^2+d^2)$,
$\Exp(c^2+d^2)\geq2 \min(\Exp(c), \Exp(d))$, and $\Exp(ac) \leq \Exp(a) +
\Exp(c)$, this gives an upper bound for $e_1$:
\[
e_1 \leq e = \Exp(\Re(u)) +\Exp(\Re(v)) -\Exp(\Re(t))
-2 \min(\Exp(\Re(v)), \Exp(\Im(v))).
\]
and a suitable value for $k_R$:
\begin{equation*}
k_R=\left\{
\begin{array}{l l}
  2^7 & \mbox{if $e < 2$;}
  \\
  2^8 & \mbox{if $e = 2$}
  \\
  2^{5 + e} & \mbox{else.}
\end{array}
\right.
\end{equation*}

\subsection {\texttt {mpc\_pow}}

The main issue for the power function is to be able to recognize when the
real or imaginary parts of $x^y$ might be exact, since in that case
Ziv's strategy will loop infinitely.

Gelfond-Schneider's theorem states that if $x$ is an algebraic number
different from $0$ and $1$ and $y$ is an algebraic non-rational number,
then $x^y$ is transcendental.
Since all exact real floating-point numbers are rational and thus all exact
complex floating point numbers are algebraic, $x^y$ can only be exact
when $y$ is rational, that is, $y$ has zero imaginary part.
Unfortunately, Gelfond-Schneider's theorem says nothing about the real and
imaginary parts of $x^y$: $x^y$ may be transcendental, but its real part
or imaginary part may be an exact (binary) floating-point number. This
may happen, for instance, when $x$ and $y$ are real; we conjecture that
when $y$ is not real, then $x^y$ is not exact.
\begin{lemma}
Let $x = c + i d$ with $c, d$ integers. Assume $x$ is an exact square
$x = (a + i b)^2$. If one of $a, b$ is a dyadic integer, i.e., of the form
$m \cdot 2^e$, then both $a$ and $b$ are dyadic integers.
\end{lemma}
\begin{proof}
Assume for example $a$ is a dyadic integer, and $b$ is not, thus 
$a = m \cdot 2^e$ with $m, e$ integers, and $b$ cannot be written in that form.
Either $b$ is rational,
with a denominator which is not a power of two, or $b$ is irrational. In the
second case, $d = 2 a b$ would be irrational two; in the first case,
$a^2 - b^2$ will have a non-power-of-two denominator.
The case where $a$ is not a dyadic integer and $b$ is a dyadic integer
is symmetric.
\end{proof}

Assume $y = m \cdot 2^e$, with $m$ an odd integer, and $e$ an integer.
(The case $m=0$ is trivial.)
If $e > 0$, then $x^y$ is trivially exact.
If $e < 0$, Lemma~\ref{lemma1} shows that exact powers can only occur when
$x^{2^e}$ is itself exact.
It thus suffices to recognize exact squares, as in the real case.

If $x^y$ is known to be inexact, then we approximate it using
$\exp(y \log x)$ and Ziv's strategy.
After computing an integer $q$ such that $|y \log x| \leq 2^q$, we first
approximate $y \log x$ with precision $p + q$, and then
$\exp(y \log x)$ with precision $p \geq 4$, all with rounding
to nearest.
Let $\tilde{s} = \circ_{p+q}(\log x)$,
we have $\tilde{s} = (\log x) (1 + \theta_1)$
with $\theta_1$ a complex number of norm $\leq 2^{-p-q}$.
Let $\tilde{t} = \circ_{p+q}(y \tilde{s})$, then
$\tilde{t} = y \tilde{s} (1 + \theta_2) = (y \log x) (1 + \theta_3)^2$,
where $\theta_2, \theta_3$ are complex numbers of norm $\leq 2^{-p-q}$,
thus $|\tilde{t} - y \log x| \leq 2.5 \cdot 2^{-p}$ for $q \geq -3$.
Now $\tilde{u} = \circ_p(\exp(\tilde{t})) =
x^y \exp(2.5 \cdot 2^{-p}) (1 + \theta_4) = x^y (1 + 4 \theta_5)$,
with $\theta_4, \theta_5$ complex numbers of norm $\leq 2^{-p}$.

\begin{lemma} \label{lemma1}
Let $x$ be a complex number with real and imaginary parts which are
dyadic integers, i.e., of the form $\lambda \cdot 2^{\mu}$ with
$\lambda, \mu$ integers. We call such a complex number a 
\emph{dyadic complex}.
If $m, e$ are integers, $m$ odd,
$x^{m 2^e}$ is a dyadic complex if and only if $x^{2^e}$ is a dyadic complex.
\end{lemma}
\begin{proof}
If $x^{2^e}$ is a dyadic complex, then $x^{m 2^e}$ is trivially a
dyadic complex too, since raising a dyadic complex to an integer power
gives a dyadic complex.

Now assume $x^{m 2^e}$ is a dyadic complex. If $e \geq 0$ then $x^{2^e}$ is
a dyadic complex too, thus we can assume $e < 0$.

First assume $e=-1$.
Write $x = (a + i b) \cdot 2^c$ with $a, b, c$ integers.
Let $t = \sqrt{a + i b}$, then its $m$-th power
can be written $t^m = t^{2k} \cdot t$ where $m=2k+1$.
This shows that if $t$ is not a Gaussian integer, neither is $t^m$, since
$t^{2k}$ is a Gaussian integer. This completes the proof for $e=-1$.

We finish the proof by recurrence on $e$. Assume $e \leq -2$.
Write $x' = x^{1/2}$. By hypothesis, $x^{m 2^e}$ is a dyadic complex,
thus $x^{m/2} = x'^m$ is a dyadic complex too; the above argument shows that
$x'$ is necessarily a dyadic complex.
Now the argument for $e=-1$ applies to $x'$, and shows
that $\sqrt{x'}$ is a dyadic complex. Thus we can apply the induction
hypothesis to $x'$ and $e+1$: if $x'^{m 2^{e+1}} = x^{m 2^e}$ is a dyadic
complex, then $x'^{2^{e+1}} = x^{2^e}$ is a dyadic complex.
\end{proof}

\bibliographystyle{acm}
\bibliography{algorithms}

\end {document}
